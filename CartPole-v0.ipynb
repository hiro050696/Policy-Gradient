{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Policy Gradient\n",
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count                   \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cài đặt các thông số cho quá trình learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gamma = 0.99      # Hệ số discount reward\n",
    "lr = 1e-2         # Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[543]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')  # Load môi trường CartPole-v0\n",
    "env.seed(543)                  # Start state của cartpole là giống nhau trên tất cả các episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xây dựng Policy  \n",
    "Policy là 1 Feedforward neural network có đầu vào là state mà agent nhận được, đầu ra là phân bố xác suất actions mà agent sẽ lựa chọn để thực hiện.\n",
    "**Architecture:** state(1x4) -> Layer(4x128) -> ReLU -> Layer(128x2) -> Softmax -> actions_distribution(1x2)  \n",
    "**Optimization:** Adam  \n",
    "**Loss:** MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(4,128)\n",
    "        self.affine2 = nn.Linear(128,2)\n",
    "\n",
    "        self.saved_log_probs = []       # reward và log() sử dụng cho training\n",
    "        self.rewards = []               # ....\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.affine1(x))\n",
    "        x = F.softmax(self.affine2(x),dim=1)\n",
    "        return x\n",
    "policy = Policy()                                 \n",
    "optimizer = optim.Adam(policy.parameters(),lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xây dựng hàm lựa chọn action cho agent khi biết đầu vào là state. Policy sẽ dự đoán phân bố xác suất của hành động, sau đó chúng ta sẽ lựa chọn hành động ngẫu nhiên dựa trên phân bố đó, đồng thời lưu trữ giá trị $ \\log() $ của xác suất hành động được chọn phục vụ cho việc update Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)       # numpy -> tensor\n",
    "    probs = policy(Variable(state))                            # Dự đoán phân bố xác suất của hành động \n",
    "    action = np.random.choice([0,1],p=probs.data[0].numpy())   # Lựa chọn hành động ngẫu nhiên dựa vào phân bố trên \n",
    "    policy.saved_log_probs.append(torch.log(probs[0][action])) # Lưu trữ log()\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xây dựng hàm updatePolicy. Sau mỗi episode, chúng ta nhận được 2 chuỗi rewards và saved_log_probs của mỗi action. Chúng ta sẽ tiến hành cập nhật Policy dựa vào công thức:  \n",
    "\n",
    "$$\\theta = \\theta + G_t \\triangledown_\\theta log\\pi(a_t|s_t,\\theta)$$\n",
    "\n",
    "Do Policy được thay thế bằng Neural Network nên hàm loss được xác định như sau:\n",
    "\n",
    "$$loss = - G_t \\triangledown_\\theta log\\pi(a_t|s_t,\\theta) $$\n",
    "\n",
    "$ G_t $ là reward của action được chọn sau khi đã discounted trên toàn bộ episode, được tính theo công thức:\n",
    "\n",
    "$$G_t = r_t + \\gamma G_{t+1}$$\n",
    "\n",
    "$ r_t $ là reward nhận được ở action t, $ G_{end} $ = $ r_{end} $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def finish_episode():\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    rewards = []\n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r + gamma * R               # discount reward\n",
    "        rewards.insert(0, R)                       \n",
    "    rewards = torch.Tensor(rewards)                                                      # numpy -> tensor\n",
    "    rewards = (rewards - rewards.mean()) / (rewards.std() + np.finfo(np.float32).eps)    # rewards normalization\n",
    "    for log_prob, reward in zip(policy.saved_log_probs, rewards):\n",
    "        policy_loss.append(-log_prob * reward)                                           # loss caculation\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = torch.cat(policy_loss).sum()      # sum up loss\n",
    "    policy_loss.backward()                          # backprop\n",
    "    optimizer.step()\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    for i_episode in count(1):\n",
    "        if i_episode % 10 == 0: print(i_episode)\n",
    "        state = env.reset()                                   # start a new game\n",
    "        for t in range(10000):\n",
    "            action = select_action(state)                     # select action\n",
    "            state,reward,done,_ = env.step(action)            # execute action\n",
    "            env.render()                                      # show GUI\n",
    "            policy.rewards.append(reward)                     # save reward\n",
    "            if done:\n",
    "                break\n",
    "        finish_episode()                                      # update policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kết quả sau khi thực hiện được 60 episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt Text](https://media.giphy.com/media/xULW8A8DeiacECjLiw/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
